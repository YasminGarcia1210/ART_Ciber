# An√°lisis del Notebook: art.ipynb

## Resumen General

Este notebook implementa un **ataque adversarial usando el m√©todo Carlini-Wagner L2** contra un modelo de clasificaci√≥n de im√°genes basado en Vision Transformer (ViT). El objetivo es generar ejemplos adversariales que enga√±en al modelo para clasificar im√°genes falsas (fake) como reales (real).

---

## Estructura del Notebook

### 1. **Importaciones y Configuraci√≥n Inicial**

```python
from art.attacks.evasion import CarliniL2Method, HopSkipJump
from art.estimators.classification import PyTorchClassifier, BlackBoxClassifier
import torch
import timm
from torchvision import transforms
from PIL import Image
```

**Prop√≥sito:**
- Importa la biblioteca **ART (Adversarial Robustness Toolbox)** para ataques adversariales
- Usa **PyTorch** y **timm** para el modelo de Vision Transformer
- Importa herramientas de transformaci√≥n de im√°genes

**‚ùå ERROR IDENTIFICADO:**
```
ModuleNotFoundError: No module named 'art'
```

---

### 2. **Configuraci√≥n del Dispositivo**

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)  # Output: cuda
```

**Prop√≥sito:**
- Verifica disponibilidad de GPU
- El notebook se ejecut√≥ en CUDA (GPU disponible)

---

### 3. **Carga del Modelo Vision Transformer**

```python
target_model = timm.create_model('vit_base_patch16_224.augreg_in21k_ft_in1k', pretrained=True)
target_model.head = torch.nn.Linear(target_model.head.in_features, 2)
target_model = target_model.to(device)
target_model.load_state_dict(torch.load("../weights/vit_teacher.pth"))
target_model.eval()
```

**Prop√≥sito:**
- Carga un Vision Transformer pre-entrenado (ViT-Base, patch 16x16, 224x224)
- **Modifica la cabeza del modelo** para clasificaci√≥n binaria (2 clases: fake/real)
- Carga pesos personalizados desde `../weights/vit_teacher.pth`
- Coloca el modelo en modo evaluaci√≥n

**Arquitectura del modelo:**
- VisionTransformer con 12 bloques de atenci√≥n
- Embedding dimension: 768
- MLP dimension: 3072
- 2 clases de salida (fake/real)

**‚ö†Ô∏è Advertencia:**
- Se us√≥ `torch.load` con `weights_only=False`, lo cual puede ser inseguro

---

### 4. **Configuraci√≥n del Clasificador ART**

```python
labels = {}
labels[0] = "fake"
labels[1] = "real"

classifier = PyTorchClassifier(
    model = target_model,
    loss = torch.nn.CrossEntropyLoss(),
    nb_classes = len(labels),  # 2 clases
    input_shape = (3, 224, 224)
)
```

**Prop√≥sito:**
- Envuelve el modelo PyTorch en un clasificador compatible con ART
- Define las etiquetas: 0=fake, 1=real
- Especifica la forma de entrada (3 canales RGB, 224x224 p√≠xeles)

---

### 5. **Creaci√≥n del Ataque Carlini-Wagner L2**

```python
attack = CarliniL2Method(classifier)
```

**Prop√≥sito:**
- Inicializa el ataque **Carlini-Wagner L2 (C&W)**
- Este ataque es uno de los m√°s efectivos para generar ejemplos adversariales
- Busca perturbaciones m√≠nimas que enga√±en al modelo

**¬øQu√© es el ataque C&W L2?**
- Minimiza la distancia L2 entre la imagen original y la adversarial
- Optimiza para encontrar la perturbaci√≥n m√°s peque√±a que cambie la clasificaci√≥n
- Es un ataque de caja blanca (white-box) que requiere acceso al gradiente

---

### 6. **Generaci√≥n del Ejemplo Adversarial**

```python
img = Image.open("../static/fake/0001_00_00_01_0.jpg")

# Preprocesamiento est√°ndar de ImageNet
preprocess = transforms.Compose([
    transforms.Resize(256), 
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# Para revertir la normalizaci√≥n
unnormalize = transforms.Normalize(
   mean= [-m/s for m, s in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])],
   std= [1/s for s in [0.229, 0.224, 0.225]]
)

img_tensor = preprocess(img).unsqueeze(0)
adversarial_example = attack.generate(img_tensor.numpy())
```

**Prop√≥sito:**
- Carga una imagen **fake** del dataset
- Aplica el preprocesamiento est√°ndar de ImageNet
- Genera el ejemplo adversarial usando el ataque C&W
- La imagen original es de clase "fake" (0)

**Pipeline:**
1. Redimensionar a 256x256
2. Recorte central a 224x224
3. Convertir a tensor
4. Normalizar con medias/desviaciones de ImageNet

---

### 7. **Verificaci√≥n del Ataque**

```python
adv_tensor = torch.from_numpy(adversarial_example).to(device)
output = target_model(adv_tensor)

print(f"Output index:\n---------------\n{output[0].argmax()}\n")
print(f"Output label:\n---------------\n{labels[output[0].argmax().item()]}\n")
```

**Salida:**
```
Output index:
---------------
1

Output label:
---------------
real
```

**‚úÖ Resultado del Ataque:**
- El modelo clasific√≥ el ejemplo adversarial como **"real" (clase 1)**
- **El ataque fue exitoso**: la imagen fake fue clasificada como real
- Esto demuestra la vulnerabilidad del modelo a ataques adversariales

---

### 8. **Guardar la Imagen Adversarial**

```python
def tensor_to_pil(img_tensor):    
    unnormed_tensor = unnormalize(img_tensor)
    return transforms.functional.to_pil_image(unnormed_tensor[0])

masked_pil = tensor_to_pil(adv_tensor)
masked_pil.save(fp="../static/adversarial/output_art.png")
```

**Prop√≥sito:**
- Convierte el tensor adversarial de vuelta a imagen PIL
- Desnormaliza la imagen
- Guarda la imagen adversarial en `../static/adversarial/output_art.png`

---

## üî¥ An√°lisis del Error

### Error Principal

```
ModuleNotFoundError: No module named 'art'
```

**Causa:**
- La biblioteca `adversarial-robustness-toolbox` (ART) no est√° instalada en el entorno

### Soluci√≥n

Instalar la biblioteca ART:

```bash
pip install adversarial-robustness-toolbox
```

O si usas conda:

```bash
conda install -c conda-forge adversarial-robustness-toolbox
```

### Dependencias Adicionales

Aseg√∫rate de tener todas las dependencias instaladas:

```bash
pip install torch torchvision timm pillow numpy
pip install adversarial-robustness-toolbox
```

---

## ‚ö†Ô∏è Advertencia de Seguridad

El notebook tiene esta advertencia:

```
FutureWarning: You are using `torch.load` with `weights_only=False`
```

**Recomendaci√≥n:**
- Cambiar a `torch.load(..., weights_only=True)` si es posible
- O usar `torch.serialization.add_safe_globals` para permitir objetos espec√≠ficos

**C√≥digo corregido:**
```python
target_model.load_state_dict(
    torch.load(
        "../weights/vit_teacher.pth",
        weights_only=True  # M√°s seguro
    )
)
```

---

## üìä Flujo Completo del Ataque

```mermaid
graph TD
    A[Imagen Original: fake] --> B[Preprocesamiento ImageNet]
    B --> C[Normalizaci√≥n]
    C --> D[Ataque Carlini-Wagner L2]
    D --> E[Generaci√≥n de Perturbaci√≥n]
    E --> F[Ejemplo Adversarial]
    F --> G[Inferencia en Modelo ViT]
    G --> H[Predicci√≥n: real]
    F --> I[Guardar Imagen]
    I --> J[../static/adversarial/output_art.png]
```

---

## üéØ Conclusiones

1. **Funcionalidad:** El notebook implementa un ataque adversarial efectivo contra un clasificador ViT de im√°genes fake/real

2. **√âxito del Ataque:** El ataque C&W logr√≥ enga√±ar al modelo para clasificar una imagen fake como real

3. **Error Cr√≠tico:** Falta instalar `adversarial-robustness-toolbox`

4. **Riesgo de Seguridad:** Uso de `torch.load` con `weights_only=False`

5. **Implicaciones:** Demuestra la vulnerabilidad de los modelos de deep learning a ataques adversariales, incluso en arquitecturas avanzadas como Vision Transformers

---

## üîß Pr√≥ximos Pasos

1. Instalar ART: `pip install adversarial-robustness-toolbox`
2. Actualizar el c√≥digo de carga de pesos para mayor seguridad
3. Ejecutar el notebook completo
4. Comparar visualmente la imagen original vs. la adversarial
5. Evaluar otras m√©tricas de ataque (distancia L2, tasa de √©xito, etc.)
